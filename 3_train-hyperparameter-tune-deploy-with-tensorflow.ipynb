{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.png)"
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "bf74d2e9-2708-49b1-934b-e0ede342f475"
        }
      },
      "cell_type": "markdown",
      "source": "# Training, hyperparameter tune, and deploy with TensorFlow\n\n## Introduction\nThis tutorial shows how to train a simple deep neural network using the MNIST dataset and TensorFlow on Azure Machine Learning. MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of `28x28` pixels, representing number from 0 to 9. The goal is to create a multi-class classifier to identify the digit each image represents.\n\nFor more information about the MNIST dataset, please visit [Yan LeCun's website](http://yann.lecun.com/exdb/mnist/).\n\n## Prerequisite:\n* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n    * install the AML SDK\n    * create a workspace and its configuration file (`config.json`)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's get started. First let's import some Python libraries."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install -U azureml-sdk[automl,explain]\n!pip install -U scikit-image",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "c377ea0c-0cd9-4345-9be2-e20fb29c94c3"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "edaa7f2f-2439-4148-b57a-8c794c0945ec"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml\nfrom azureml.core import Workspace\n\n# check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Diagnostics\nOpt-in diagnostics for better experience, quality, and security of future releases."
    },
    {
      "metadata": {
        "tags": [
          "Diagnostics"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.telemetry import set_diagnostics_collection\n\nset_diagnostics_collection(send_diagnostics=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Initialize workspace\nInitialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace.from_config()\nprint('Workspace name: ' + ws.name, \n      'Azure region: ' + ws.location, \n      'Subscription id: ' + ws.subscription_id, \n      'Resource group: ' + ws.resource_group, sep = '\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "59f52294-4a25-4c92-bab8-3b07f0f44d15"
        }
      },
      "cell_type": "markdown",
      "source": "## Create an Azure ML experiment\nLet's create an experiment named \"tf-mnist\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "bc70f780-c240-4779-96f3-bc5ef9a37d59"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\n\nscript_folder = './src/tf-mnist'\nos.makedirs(script_folder, exist_ok=True)\n\nexp = Experiment(workspace=ws, name='tf-mnist')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "defe921f-8097-44c3-8336-8af6700804a7"
        }
      },
      "cell_type": "markdown",
      "source": "## Download MNIST dataset\nIn order to train on the MNIST dataset we will first need to download it from Yan LeCun's web site directly and save them in a `data` folder locally."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib\n\nos.makedirs('./data/mnist', exist_ok=True)\n\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename = './data/mnist/train-images.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename = './data/mnist/train-labels.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename = './data/mnist/test-images.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename = './data/mnist/test-labels.gz')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "c3f2f57c-7454-4d3e-b38d-b0946cf066ea"
        }
      },
      "cell_type": "markdown",
      "source": "## Show some sample images\nLet's load the downloaded compressed file into numpy arrays using some utility functions included in the `utils.py` library file from the current folder. Then we use `matplotlib` to plot 30 random images from the dataset along with their labels."
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "396d478b-34aa-4afa-9898-cdce8222a516"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "from src.utils import load_data\n\n# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\nX_train = load_data('./data/mnist/train-images.gz', False) / 255.0\ny_train = load_data('./data/mnist/train-labels.gz', True).reshape(-1)\n\nX_test = load_data('./data/mnist/test-images.gz', False) / 255.0\ny_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)\n\ncount = 0\nsample_size = 30\nplt.figure(figsize = (16, 6))\nfor i in np.random.permutation(X_train.shape[0])[:sample_size]:\n    count = count + 1\n    plt.subplot(1, sample_size, count)\n    plt.axhline('')\n    plt.axvline('')\n    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n    plt.imshow(X_train[i].reshape(28, 28), cmap = plt.cm.Greys)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Upload MNIST dataset to default datastore \nA [datastore](https://docs.microsoft.com/azure/machine-learning/service/how-to-access-data) is a place where data can be stored that is then made accessible to a Run either by means of mounting or copying the data to the compute target. A datastore can either be backed by an Azure Blob Storage or and Azure File Share (ADLS will be supported in the future). For simple data handling, each workspace provides a default datastore that can be used, in case the data is not already in Blob Storage or File Share."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds = ws.get_default_datastore()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this next step, we will upload the training and test set into the workspace's default datastore, which we will then later be mount on an `AmlCompute` cluster for training."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Get default AmlCompute\nYou can create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you use default `AmlCompute` as your training compute resource."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cts = ws.compute_targets\ncompute_target = cts['cpucluster']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that you have retrieved the compute target, let's see what the workspace's `compute_targets` property returns."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "compute_targets = ws.compute_targets\nfor name, ct in compute_targets.items():\n    print(name, ct.type, ct.provisioning_state)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Copy the training files into the script folder\nThe TensorFlow training script is already created for you. You can simply copy it into the script folder, together with the utility library used to load compressed data file into numpy array."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import shutil\n\n# the training logic is in the tf_mnist.py file.\nshutil.copy('./src/tf_mnist.py', script_folder)\n\n# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\nshutil.copy('./src/utils.py', script_folder)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "2039d2d5-aca6-4f25-a12f-df9ae6529cae"
        }
      },
      "cell_type": "markdown",
      "source": "## Construct neural network in TensorFlow\nIn the training script `tf_mnist.py`, it creates a very simple DNN (deep neural network), with just 2 hidden layers. The input layer has 28 * 28 = 784 neurons, each representing a pixel in an image. The first hidden layer has 300 neurons, and the second hidden layer has 100 neurons. The output layer has 10 neurons, each representing a targeted label from 0 to 9.\n\n![DNN](src/nn.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Azure ML concepts  \nPlease note the following three things in the code below:\n1. The script accepts arguments using the argparse package. In this case there is one argument `--data_folder` which specifies the file system folder in which the script can find the MNIST data\n```\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_folder')\n```\n2. The script is accessing the Azure ML `Run` object by executing `run = Run.get_context()`. Further down the script is using the `run` to report the training accuracy and the validation accuracy as training progresses.\n```\n    run.log('training_acc', np.float(acc_train))\n    run.log('validation_acc', np.float(acc_val))\n```\n3. When running the script on Azure ML, you can write files out to a folder `./outputs` that is relative to the root directory. This folder is specially tracked by Azure ML in the sense that any files written to that folder during script execution on the remote target will be picked up by Run History; these files (known as artifacts) will be available as part of the run history record."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The next cell will print out the training code for you to inspect it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open(os.path.join(script_folder, './tf_mnist.py'), 'r') as f:\n    print(f.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create TensorFlow estimator\nNext, we construct an `azureml.train.dnn.TensorFlow` estimator object, use the Batch AI cluster as compute target, and pass the mount-point of the datastore to the training code as a parameter.\n\nThe TensorFlow estimator is providing a simple way of launching a TensorFlow training job on a compute target. It will automatically provide a docker image that has TensorFlow installed -- if additional pip or conda packages are required, their names can be passed in via the `pip_packages` and `conda_packages` arguments and they will be included in the resulting docker.\n\nThe TensorFlow estimator also takes a `framework_version` parameter -- if no version is provided, the estimator will default to the latest version supported by AzureML. Use `TensorFlow.get_supported_versions()` to get a list of all versions supported by your current SDK version or see the [SDK documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn?view=azure-ml-py) for the versions supported in the most current release."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.dnn import TensorFlow\n\nscript_params = {\n    '--data-folder': ws.get_default_datastore().as_mount(),\n    '--batch-size': 50,\n    '--first-layer-neurons': 300,\n    '--second-layer-neurons': 100,\n    '--learning-rate': 0.01\n}\n\nest = TensorFlow(source_directory=script_folder,\n                 script_params=script_params,\n                 compute_target=compute_target,\n                 entry_script='tf_mnist.py', \n                 use_gpu=False, \n                 framework_version='1.13')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Submit job to run\nSubmit the estimator to an Azure ML experiment to kick off the execution."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = exp.submit(est)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Monitor the Run\nAs the Run is executed, it will go through the following stages:\n1. Preparing: A docker image is created matching the Python environment specified by the TensorFlow estimator and it will be uploaded to the workspace's Azure Container Registry. This step will only happen once for each Python environment -- the container will then be cached for subsequent runs. Creating and uploading the image takes about **5 minutes**. While the job is preparing, logs are streamed to the run history and can be viewed to monitor the progress of the image creation.\n\n2. Scaling: If the compute needs to be scaled up (i.e. the Batch AI cluster requires more nodes to execute the run than currently available), the cluster will attempt to scale up in order to make the required amount of nodes available. Scaling typically takes about **5 minutes**.\n\n3. Running: All scripts in the script folder are uploaded to the compute target, data stores are mounted/copied and the `entry_script` is executed. While the job is running, stdout and the `./logs` folder are streamed to the run history and can be viewed to monitor the progress of the run.\n\n4. Post-Processing: The `./outputs` folder of the run is copied over to the run history\n\nThere are multiple ways to check the progress of a running job. We can use a Jupyter notebook widget. \n\n**Note: The widget will automatically update ever 10-15 seconds, always showing you the most up-to-date information about the run**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also periodically check the status of the run object, and navigate to Azure portal to monitor the run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The Run object\nThe Run object provides the interface to the run history -- both to the job and to the control plane (this notebook), and both while the job is running and after it has completed. It provides a number of interesting features for instance:\n* `run.get_details()`: Provides a rich set of properties of the run\n* `run.get_metrics()`: Provides a dictionary with all the metrics that were reported for the Run\n* `run.get_file_names()`: List all the files that were uploaded to the run history for this Run. This will include the `outputs` and `logs` folder, azureml-logs and other logs, as well as files that were explicitly uploaded to the run using `run.upload_file()`\n\nBelow are some examples -- please run through them and inspect their output. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.get_details()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.get_metrics()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.get_file_names()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Plot accuracy over epochs\nSince we can retrieve the metrics from the run, we can easily make plots using `matplotlib` in the notebook. Then we can add the plotted image to the run using `run.log_image()`, so all information about the run is kept together."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nos.makedirs('./imgs', exist_ok=True)\nmetrics = run.get_metrics()\n\nplt.figure(figsize = (13,5))\nplt.plot(metrics['validation_acc'], 'r-', lw=4, alpha=.6)\nplt.plot(metrics['training_acc'], 'b--', alpha=0.5)\nplt.legend(['Full evaluation set', 'Training set mini-batch'])\nplt.xlabel('epochs', fontsize=14)\nplt.ylabel('accuracy', fontsize=14)\nplt.title('Accuracy over Epochs', fontsize=16)\nrun.log_image(name='acc_over_epochs.png', plot=plt)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Download the saved model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the training script, a TensorFlow `saver` object is used to persist the model in a local folder (local to the compute target). The model was saved to the `./outputs` folder on the disk of the Batch AI cluster node where the job is run. Azure ML automatically uploaded anything written in the `./outputs` folder into run history file store. Subsequently, we can use the `Run` object to download the model files the `saver` object saved. They are under the the `outputs/model` folder in the run history file store, and are downloaded into a local folder named `model`. Note the TensorFlow model consists of four files in binary format and they are not human-readable."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a model folder in the current directory\nos.makedirs('./model', exist_ok=True)\n\nfor f in run.get_file_names():\n    if f.startswith('outputs/model'):\n        output_file_path = os.path.join('./model', f.split('/')[-1])\n        print('Downloading from {} to {} ...'.format(f, output_file_path))\n        run.download_file(name=f, output_file_path=output_file_path)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Predict on the test set\nNow load the saved TensorFlow graph, and list all operations under the `network` scope. This way we can discover the input tensor `network/X:0` and the output tensor `network/output/MatMul:0`, and use them in the scoring script in the next step.\n\nNote: if your local TensorFlow version is different than the version running in the cluster where the model is trained, you might see a \"compiletime version mismatch\" warning. You can ignore it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\n\ntf.reset_default_graph()\n\nsaver = tf.train.import_meta_graph(\"./model/mnist-tf.model.meta\")\ngraph = tf.get_default_graph()\n\nfor op in graph.get_operations():\n    if op.name.startswith('network'):\n        print(op.name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Feed test dataset to the persisted model to get predictions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# input tensor. this is an array of 784 elements, each representing the intensity of a pixel in the digit image.\nX = tf.get_default_graph().get_tensor_by_name(\"network/X:0\")\n# output tensor. this is an array of 10 elements, each representing the probability of predicted value of the digit.\noutput = tf.get_default_graph().get_tensor_by_name(\"network/output/MatMul:0\")\n\nwith tf.Session() as sess:\n    saver.restore(sess, './model/mnist-tf.model')\n    k = output.eval(feed_dict={X : X_test})\n# get the prediction, which is the index of the element that has the largest probability value.\ny_hat = np.argmax(k, axis=1)\n\n# print the first 30 labels and predictions\nprint('labels:  \\t', y_test[:30])\nprint('predictions:\\t', y_hat[:30])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Calculate the overall accuracy by comparing the predicted value against the test set."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Accuracy on the test set:\", np.average(y_hat == y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Intelligent hyperparameter tuning\nWe have trained the model with one set of hyperparameters, now let's how we can do hyperparameter tuning by launching multiple runs on the cluster. First let's define the parameter space using random sampling."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\nfrom azureml.train.hyperdrive import choice, loguniform\n\nps = RandomParameterSampling(\n    {\n        '--batch-size': choice(25, 50, 100),\n        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n        '--second-layer-neurons': choice(10, 50, 200, 500),\n        '--learning-rate': loguniform(-6, -1)\n    }\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we will create a new estimator without the above parameters since they will be passed in later. Note we still need to keep the `data-folder` parameter since that's not a hyperparamter we will sweep."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "est = TensorFlow(source_directory=script_folder,\n                 script_params={'--data-folder': ws.get_default_datastore().as_mount()},\n                 compute_target=compute_target,\n                 entry_script='tf_mnist.py', \n                 use_gpu=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we will define an early termnination policy. The `BanditPolicy` basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we are ready to configure a run configuration object, and specify the primary metric `validation_acc` that's recorded in your training runs. If you go back to visit the training script, you will notice that this value is being logged after every epoch (a full batch set). We also want to tell the service that we are looking to maximizing this value. We also set the number of samples to 20, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "htc = HyperDriveConfig(estimator=est, \n                       hyperparameter_sampling=ps, \n                       policy=policy, \n                       primary_metric_name='validation_acc', \n                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                       max_total_runs=8,\n                       max_concurrent_runs=4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, let's launch the hyperparameter tuning job."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "htr = exp.submit(config=htc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can use a run history widget to show the progress. Be patient as this might take a while to complete."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "RunDetails(htr).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "htr.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Find and register best model\nWhen all the jobs finish, we can find out the one that has the highest accuracy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run = htr.get_best_run_by_primary_metric()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's list the model files uploaded during the run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(best_run.get_file_names())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can then register the folder (and all files in it) as a model named `tf-dnn-mnist` under the workspace for deployment."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = best_run.register_model(model_name='tf-dnn-mnist', model_path='outputs/model')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "minxia"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "msauthor": "minxia"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}