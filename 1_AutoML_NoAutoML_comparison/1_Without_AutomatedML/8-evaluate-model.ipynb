{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport pickle\nimport os\nimport sys",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_name = sys.argv[1]\n\naml_dir = './data/'\n\n# set forecast horizon\nH = 6\n\n# run_logger = get_azureml_logger()\n# run_logger.log('amlrealworld.timeseries.evaluate-model','true')\n\nrun_logger = np.log('amlrealworld.timeseries.evaluate-model','true')\n#run_logger.log('amlrealworld.timeseries.evaluate-model','true')\n\ndef generate_forecasts(test_df):\n    '''\n    The models trained in notebooks 2-7 are 'one-step' forecasts\n    because they are trained to predict one time period into the \n    future. Here, we use the trained model recursively to predict\n    multiple future time steps. At each iteration from time t+1\n    to the forecast horizon H, the predictions from the previous\n    steps become the lagged demand input features for subsequent\n    predictions.\n    '''\n    \n    predictions_df = test_df.copy()\n    X_test = test_df.copy().drop(['demand', 'timeStamp'], axis=1)\n    \n    # Iterate over future time steps\n    for n in range(1, H+1):\n        predictions_df['pred_t+'+str(n)] = model.predict(X_test)\n        \n        # shift lagged demand features...\n        shift_demand_features(X_test)\n        \n        # ...and replace demand_lag1 with latest prediction\n        X_test['demand_lag1'] = predictions_df['pred_t+'+str(n)]\n        \n    return predictions_df\n\n\ndef shift_demand_features(df):\n    for i in range(H, 1, -1):\n        df['demand_lag'+str(i)] = df['demand_lag'+str(i-1)]\n\n\ndef evaluate_forecast(predictions_df, n):\n    '''\n    Compute forecast performance metrics for every n step ahead\n    '''\n\n    y_true = predictions_df['demand']\n    y_pred = predictions_df['pred_t+'+str(n)]\n    error = y_pred - y_true\n    \n    metrics = {}\n    \n    # forecast bias\n    metrics['ME'] = error.mean()\n    metrics['MPE'] = 100 * (error / y_true).mean()\n    \n    # forecast error\n    #MSE = mean_squared_error(y_true, y_pred)\n    metrics['MSE'] = (error**2).mean()\n    metrics['RMSE'] = metrics['MSE']**0.5\n    metrics['MAPE'] = 100 * (error.abs() / y_true).mean()\n    metrics['sMAPE'] = 200 * (error.abs() / y_true).mean()\n    \n    # relative error\n    naive_pred = predictions_df['demand_lag'+str(n)]\n    naive_error = naive_pred - y_true\n    RE = error / naive_error\n    metrics['MAPE_base'] = 100 * (naive_error.abs() / y_true).mean()\n    metrics['MdRAE'] = np.median(RE.abs())\n    \n    return metrics\n\n\ndef plot_metric(metric, performance_metrics):\n    '''\n    Plots metrics over forecast period t+1 to t+H\n    '''\n    plt_series = performance_metrics.stack()[metric]\n    fig = plt.figure(figsize=(6, 4), dpi=75)\n    plt.plot(plt_series.index, plt_series)\n    plt.xlabel(\"Forecast t+n\")\n    plt.ylabel(metric)\n    fig.savefig(os.path.join('.', 'outputs', metric + '.png'), bbox_inches='tight')\n\n\nif __name__=='__main__':\n    \n    # run_logger.log(\"Model Name\", model_name)\n\n    # load the test set\n    test = pd.read_csv(os.path.join(aml_dir, 'nyc_demand_test.csv'), parse_dates=['timeStamp'])\n\n    # Load trained model pipeline\n    with open(os.path.join(aml_dir, model_name + '.pkl'), 'rb') as f:\n        model = pickle.load(f)\n\n    # generate forecasts on the test set\n    predictions_df = generate_forecasts(test)\n\n    # calculate model performance metrics\n    performance_metrics = pd.DataFrame.from_dict({1:evaluate_forecast(predictions_df, 1),\n                                                2:evaluate_forecast(predictions_df, 2),\n                                                3:evaluate_forecast(predictions_df, 3),\n                                                4:evaluate_forecast(predictions_df, 4),\n                                                5:evaluate_forecast(predictions_df, 5),\n                                                6:evaluate_forecast(predictions_df, 6)})\n\n    # Compute and log average of metrics over the forecast horizon\n    horizon_mean = performance_metrics.mean(axis=1)\n    for metric, value in horizon_mean.iteritems():\n        run_logger.log(metric + '_horizon', value)\n\n    # Log the t+1 forecast metrics\n    for metric, value in performance_metrics[1].iteritems():\n        run_logger.log(metric, value)\n        \n    # Plot metrics over forecast period. View the output in Run History to view.\n    plot_metric('MAPE', performance_metrics)\n    plot_metric('MdRAE', performance_metrics)\n    plot_metric('MPE', performance_metrics)\n\n    # Output the predictions dataframe\n    with open(os.path.join(aml_dir, model_name + '_predictions.pkl'), 'wb') as f:\n        pickle.dump(predictions_df, f)\n\n    # Store the trained model in the Outputs folder.\n    with open(os.path.join('.', 'outputs', model_name + '.pkl'), 'wb') as f:    \n        pickle.dump(model, f)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "return arrays must be of ArrayType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-88cbddbbfdd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# run_logger.log('amlrealworld.timeseries.evaluate-model','true')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrun_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amlrealworld.timeseries.evaluate-model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#run_logger.log('amlrealworld.timeseries.evaluate-model','true')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: return arrays must be of ArrayType"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}