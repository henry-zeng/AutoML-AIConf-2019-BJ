{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/classification-with-onnx/auto-ml-classification-with-onnx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz - MNIST ONNX Model Registration and Deployment\n",
    "_**Register best MNIST model trained by AutoML and deploy it for inferencing**_\n",
    "\n",
    "## Introduction\n",
    "In this notebook, you need to register the MNIST model you trained with AutoML in [notebook 5](https://github.com/henry-zeng/AutoML-AIConf-2019-BJ/blob/master/5_automl-classification-with-onnx.ipynb) and using what you learned in [notebook 6](https://github.com/henry-zeng/AutoML-AIConf-2019-BJ/blob/master/6_onnx-inference-mnist-deploy.ipynb) to deploy in ACI for inferencing. Below lists the key steps you need to complete:\n",
    "\n",
    "1. Setup\n",
    "2. Register your model with Azure ML\n",
    "3. Specify score and environment files\n",
    "4. Write docker file and create container image\n",
    "5. Deploy\n",
    "\n",
    "After you complete these steps, you can use the sample code we provide to verify and test the deployed web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As part of the setup you have already created an Azure ML `Workspace` object. For AutoML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for the experiment and specify the project folder.\n",
    "experiment_name = 'automl-classification-onnx'\n",
    "project_folder = './projects/automl-classification-onnx'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace Name'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register your model with Azure ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference code in notebook 6: https://github.com/henry-zeng/AutoML-AIConf-2019-BJ/blob/master/6_onnx-inference-mnist-deploy.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the registered model is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the following code to verify if your registered model is working\n",
    "'''\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from azureml.core.model import Model\n",
    "\n",
    "session = onnxruntime.InferenceSession(Model.get_model_path(model_name = 'mnist_best_onnx'), None)\n",
    "\n",
    "print(Model.get_model_path(model_name = 'mnist_best_onnx'))\n",
    "\n",
    "k = 66\n",
    "X_test_k = X_test[k].tolist()\n",
    "y_test_k = y_test[k]\n",
    "\n",
    "print(\"Ground truth:\", y_test_k)\n",
    "\n",
    "session_input = {}\n",
    "for x in session.get_inputs():\n",
    "    i = int(x.name[1:]) - 1 # x.name should be 'CXXX', in which 'XXX' indicates the column index starting from 1.\n",
    "    session_input[x.name] = np.array([X_test_k[i]]).astype('float32')\n",
    "\n",
    "r = session.run([session.get_outputs()[0].name], session_input)\n",
    "\n",
    "print(\"Prediction:  \", r[0][0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify our score and environment files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to deploy our ONNX Model on AML with inference in ONNX Runtime. We begin by writing a score.py file, which will help us run the model in our Azure ML virtual machine (VM), and then specify our environment by writing a yml file. You will also notice that we import the onnxruntime library to do runtime inference on our ONNX models (passing in input and evaluating out model's predicted output). More information on the API and commands can be found in the [ONNX Runtime documentation](https://aka.ms/onnxruntime).\n",
    "\n",
    "### Write score file\n",
    "\n",
    "A score file is what tells our Azure cloud service what to do. After initializing our model using azureml.core.model, we start an ONNX Runtime inference session to evaluate the data passed in on our function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score file below is a reference for you.\n",
    "'''\n",
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import sys\n",
    "import os\n",
    "from azureml.core.model import Model\n",
    "import time\n",
    "\n",
    "\n",
    "def init():\n",
    "    global session, input_name, output_name\n",
    "    model = Model.get_model_path(model_name = 'mnist_best_onnx')\n",
    "    session = onnxruntime.InferenceSession(model, None)\n",
    "\n",
    "\n",
    "def run(input_data):\n",
    "\n",
    "    try:\n",
    "        # load in our data, convert to readable format\n",
    "        data = np.array(json.loads(input_data)['data']).astype('float32').tolist()\n",
    "        session_input = {}\n",
    "        for x in session.get_inputs():\n",
    "            i = int(x.name[1:]) - 1 # x.name should be 'CXXX', in which 'XXX' indicates the column index starting from 1.\n",
    "            session_input[x.name] = np.array([data[i]]).astype('float32')\n",
    "\n",
    "        # start timer\n",
    "        start = time.time()\n",
    "\n",
    "        r = session.run([session.get_outputs()[0].name], session_input)\n",
    "\n",
    "        #end timer\n",
    "        end = time.time()\n",
    "\n",
    "        result_dict = {\"result\": int(r[0][0]),\n",
    "                      \"time_in_sec\": end - start}\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e)}\n",
    "\n",
    "    return result_dict\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write environment file\n",
    "\n",
    "This step creates a YAML environment file that specifies which dependencies we would like to see in our Linux Virtual Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference code in notebook 6: https://github.com/henry-zeng/AutoML-AIConf-2019-BJ/blob/master/6_onnx-inference-mnist-deploy.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Container Image\n",
    "This step will likely take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference code in notebook 6: https://github.com/henry-zeng/AutoML-AIConf-2019-BJ/blob/master/6_onnx-inference-mnist-deploy.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're all done specifying what we want our virtual machine to do. Let's configure and deploy our container image.\n",
    "\n",
    "### Deploy the container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference code in notebook 6: https://github.com/henry-zeng/AutoML-AIConf-2019-BJ/blob/master/6_onnx-inference-mnist-deploy.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success!\n",
    "\n",
    "If you've made it this far, you've deployed a working VM with a handwritten digit classifier running in the cloud using Azure ML. Congratulations!\n",
    "\n",
    "You can get the URL for the webservice with the code below. Let's now see how well our model deals with our test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aci_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some sample images in X_test\n",
    "We use `matplotlib` to plot 10 test images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "for k in np.arange(10):\n",
    "    plt.subplot(1, 15, k+1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.imshow(X_test[k].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation / prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6), frameon=False)\n",
    "plt.subplot(1, 11, 1)\n",
    "\n",
    "plt.text(x = 0, y = -30, s = \"True Label: \", fontsize = 11, color = 'black')\n",
    "plt.text(x = 0, y = -20, s = \"Result: \", fontsize = 11, color = 'black')\n",
    "plt.text(x = 0, y = -10, s = \"Inference Time: \", fontsize = 11, color = 'black')\n",
    "plt.text(x = 3, y = 14, s = \"Model Input\", fontsize = 10, color = 'black')\n",
    "plt.text(x = 6, y = 18, s = \"(28 x 28)\", fontsize = 10, color = 'black')\n",
    "plt.imshow(np.ones((28,28)), cmap=plt.cm.Greys)    \n",
    "\n",
    "\n",
    "for i in np.arange(10):\n",
    "    \n",
    "    input_data = json.dumps({'data': X_test[i].tolist()})\n",
    "    \n",
    "    \n",
    "    # predict using the deployed model\n",
    "    r = aci_service.run(input_data)\n",
    "    \n",
    "    if \"error\" in r:\n",
    "        print(\"Error:\", r['error'])\n",
    "        break\n",
    "        \n",
    "    result = r['result']\n",
    "    time_ms = np.round(r['time_in_sec'] * 1000, 2)\n",
    "    \n",
    "    ground_truth = int(y_test[i])\n",
    "    \n",
    "    # compare actual value vs. the predicted values:\n",
    "    plt.subplot(1, 11, i+2)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "\n",
    "    # use different color for misclassified sample\n",
    "    font_color = 'red' if ground_truth != result else 'black'\n",
    "    clr_map = plt.cm.gray if ground_truth != result else plt.cm.Greys\n",
    "\n",
    "    # ground truth labels are in blue\n",
    "    plt.text(x = 10, y = -30, s = ground_truth, fontsize = 18, color = 'blue')\n",
    "    \n",
    "    # predictions are in black if correct, red if incorrect\n",
    "    plt.text(x = 10, y = -20, s = result, fontsize = 18, color = font_color)\n",
    "    plt.text(x = 5, y = -10, s = str(time_ms) + ' ms', fontsize = 14, color = font_color)\n",
    "\n",
    "    \n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap = clr_map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classifying your own images!\n",
    "\n",
    "Create your own handwritten image and pass it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions take your image and format it so it can be passed\n",
    "# as input into our ONNX model\n",
    "\n",
    "import cv2\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"Convert the input image into grayscale\"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def resize_img(img_to_resize):\n",
    "    \"\"\"Resize image to MNIST model input dimensions\"\"\"\n",
    "    r_img = cv2.resize(img_to_resize, dsize=(28, 28), interpolation=cv2.INTER_AREA)\n",
    "    r_img.resize((1, 1, 28, 28))\n",
    "    return r_img\n",
    "\n",
    "def preprocess(img_to_preprocess):\n",
    "    \"\"\"Resize input images and convert them to grayscale.\"\"\"\n",
    "    if img_to_preprocess.shape == (28, 28):\n",
    "        img_to_preprocess.resize((1, 1, 28, 28))\n",
    "        return img_to_preprocess\n",
    "    \n",
    "    grayscale = rgb2gray(img_to_preprocess)\n",
    "    processed_img = resize_img(grayscale)\n",
    "    return processed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this string with your own path/test image\n",
    "# Make sure your image is square and the dimensions are equal (i.e. 100 * 100 pixels or 28 * 28 pixels)\n",
    "\n",
    "# Any PNG or JPG image file should work\n",
    "\n",
    "your_test_image = \"<path to file>\"\n",
    "\n",
    "# e.g. your_test_image = \"C:/Users/vinitra.swamy/Pictures/handwritten_digit.png\"\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "if your_test_image != \"<path to file>\":\n",
    "    img = mpimg.imread(your_test_image)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img, cmap = plt.cm.Greys)\n",
    "    print(\"Old Dimensions: \", img.shape)\n",
    "    img = preprocess(img)\n",
    "    print(\"New Dimensions: \", img.shape)\n",
    "else:\n",
    "    img = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img is None:\n",
    "    print(\"Add the path for your image data.\")\n",
    "else:\n",
    "    input_data = json.dumps({'data': img.tolist()})\n",
    "\n",
    "    try:\n",
    "        r = aci_service.run(input_data)\n",
    "        result = r['result']\n",
    "        time_ms = np.round(r['time_in_sec'] * 1000, 2)\n",
    "    except KeyError as e:\n",
    "        print(str(e))\n",
    "\n",
    "    plt.figure(figsize = (16, 6))\n",
    "    plt.subplot(1, 15,1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = -100, y = -20, s = \"Model prediction: \", fontsize = 14)\n",
    "    plt.text(x = -100, y = -10, s = \"Inference time: \", fontsize = 14)\n",
    "    plt.text(x = 0, y = -20, s = str(result), fontsize = 14)\n",
    "    plt.text(x = 0, y = -10, s = str(time_ms) + \" ms\", fontsize = 14)\n",
    "    plt.text(x = -100, y = 14, s = \"Input image: \", fontsize = 14)\n",
    "    plt.imshow(img.reshape(28, 28), cmap = plt.cm.gray)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to delete your service after you are done using it!\n",
    "\n",
    "# aci_service.delete()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "savitam"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
